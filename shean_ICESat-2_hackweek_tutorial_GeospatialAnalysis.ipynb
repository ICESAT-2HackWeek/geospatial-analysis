{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data Exploration, Analysis, and Visualization\n",
    "### Some core concepts and modern geospatial data science approaches for ICESat data analysis*\n",
    "*Not intended for billions of ATL03 points\n",
    "\n",
    "ICESat-2 hackweek  \n",
    "June 12, 2020 \n",
    "David Shean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've used the NSIDC API, pulled your granules, extracted the relevant records and fields, and done some initial QA/QC.  Let's work through some modern data science tools that can help with analysis and visualization of these point data, enabling accelerated data-driven discovery.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "* Review fundamental concepts that are common to most geospatial analysis (e.g. coordinate systems, projections, datums, geometry types)\n",
    "* Learn basic geospatial data manipulation and exploration with a relatively small, clean ICESat GLAS dataset\n",
    "* Use modern data science tools (pandas/geopandas) for data exploration, interpretation, and visualization\n",
    "* Explore options to capture elevation change using sparse laser altimetry data over mountain glaciers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial prep\n",
    "\n",
    "Open a terminal in Jupyterlab (\"+\" icon in upper left corner, then click on the terminal icon)\n",
    "\n",
    "1. Navigate to local directory where you want to store tutorial material (home directory by default):  \n",
    "`cd ~`\n",
    "\n",
    "2. Clone the repo:  \n",
    "`git clone https://github.com/ICESAT-2HackWeek/geospatial-analysis.git`\n",
    "\n",
    "3. Enter the tutorial directory:  \n",
    "`cd geospatial-analysis`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## The Scientific Python landscape\n",
    "\n",
    "* Python\n",
    "* iPython, Jupyter\n",
    "* NumPy, Pandas, Matplotlib, SciPy\n",
    "* xarray, scikit-image, scikit-learn\n",
    "\n",
    "One interpretation of this stack:\n",
    "\n",
    "![2017 Scientific Python Stack](https://ryanwingate.com/assets/images/python/python-1-1.png)\n",
    "Slide from Jake VanderPlas’s presentation at PyCon 2017, entitled “The Unexpected Effectiveness of Python in Science.”\n",
    "\n",
    "## The Geospatial Python landscape\n",
    "* GDAL/OGR, GEOS, PROJ\n",
    "* shapely, fiona, pyproj\n",
    "* geopandas, rasterio, cartopy\n",
    "\n",
    "[Insert shiny new figure here]\n",
    "\n",
    "## Complementary approaches for ICESat-2 data\n",
    "\n",
    "1. Efficient, scalable processing of huge point datasets\n",
    "    * Mostly NumPy\n",
    "    * Basic array manipulations\n",
    "    * Array size limited by memory\n",
    "2. Higher-level data science - *analysis, interpetation, and visualization*\n",
    "    * NumPy under the hood\n",
    "    * Convenience and flexibility comes with small performance hit\n",
    "    * Labels make life easier (don't need to remember integer indices)\n",
    "\n",
    "Here, we're going to explore #2 with an existing dataset.\n",
    "\n",
    "At the end of the day, most of us just want/need (x,y,z,t) with ICESat-2.\n",
    "\n",
    "As with all things in the *nix/open-source/Python world, there are always multiple approaches that can be used to accomplish the same goals.  The user must decide on an approach based on complexity, time constraints, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICESat GLAS Background\n",
    "The NASA Ice Cloud and land Elevation Satellite ([ICESat](https://icesat.gsfc.nasa.gov/icesat/)) was a NASA mission carrying the Geosciences Laser Altimeter System (GLAS) instrument: a space laser, pointed down at the Earth (and unsuspecting Earthlings).  \n",
    "\n",
    "It measured surface elevations by precisely tracking laser pulses emitted from the spacecraft at a rate of 40 Hz (a new pulse every 0.025 seconds).  These pulses traveled through the atmosphere, reflected off the surface, back up through the atmosphere, and into space, where some small fraction of that original energy was received by a telescope on the spacecraft.  The instrument electronics precisely recorded the time when these intrepid photons left the instrument and when they returned.  The position and orientation of the spacecraft was precisely known, so the two-way traveltime (and assumptions about the speed of light and propagation through the atmosphere) allowed for precise forward determination of the spot on the Earth's surface (or cloud tops, as was often the case) where the reflection occurred.  The laser spot size varied during the mission, but was ~70 m in diameter. \n",
    "\n",
    "ICESat collected billions of measurements from 2003 to 2009, and was operating in a \"repeat-track\" mode that sacrificed spatial coverage for more observations along the same ground tracks over time.  One primary science focus involved elevation change over the Earth's ice sheets.  It allowed for early measurements of full Antarctic and Greenland ice sheet elevation change, which offered a detailed look at spatial distribution and rates of mass loss, and total ice sheet contributions to sea level rise.  \n",
    "\n",
    "There were problems with the lasers during the mission, so it operated in short campaigns lasting only a few months to prolong the full mission lifetime.  While the primary measurements focused on the polar regions, many measurements were also collected over lower latitudes, to meet other important science objectives (e.g., estimating biomass in the Earth's forests, observing sea surface height/thickness over time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample GLAS dataset for CONUS\n",
    "A few years ago, I wanted to evaluate ICESat coverage of the Continental United States (CONUS).  The primary application was to extract a set of accurate control points to co-register a large set of high-resolution digital elevation modoels (DEMs) derived from satellite stereo imagery.  I wrote some Python/shell scripts to download, filter, and process all of the GLAH14 granules in parallel ([https://github.com/dshean/icesat_tools](https://github.com/dshean/icesat_tools)).\n",
    "\n",
    "The high-level workflow is here: https://github.com/dshean/icesat_tools/blob/master/glas_proc.py#L24.  These tools processed each HDF5 (H5) file and wrote out csv files containing “good” points. These csv files were concatenated to prepare the single input csv (`GLAH14_tllz_conus_lulcfilt_demfilt.csv`) that we will use for this tutorial.  \n",
    "\n",
    "The csv contains ICESat GLAS shots that passed the following filters:\n",
    "* Within some buffer (~110 km) of mapped glacier polygons from the [Randolph Glacier Inventory (RGI)](https://www.glims.org/RGI/)\n",
    "* Returns from exposed bare ground (landcover class 31) or snow/ice (12) according to a 30-m Land-use/Land-cover dataset (2011 NLCD, https://www.mrlc.gov/data?f%5B0%5D=category%3Aland%20cover)\n",
    "* Elevation values within some threshold (200 m) of elevations sampled from an external reference DEM (void-filled 1/3-arcsec [30-m] SRTM-GL1, https://lpdaac.usgs.gov/products/srtmgl1v003/), used to remove spurious points and returns from clouds.\n",
    "* Various other ICESat-specific quality flags (see comments in `glas_proc.py` for details)\n",
    "\n",
    "The final file contains a relatively small subset (~65K) of the total shots in the original GLAH14 data granules from the full mission timeline (2003-2009).  The remaining points should represent returns from the Earth's surface with reasonably high quality, and can be used for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, I thought this was an ICESat-**2** hackweek?\n",
    "\n",
    "Note that we could (and should!) swap similarly processed/filtered ATL06 points over CONUS for this tutorial.  I did not have time to do this before the hackweek, but it would make a great project (nudge).  After opening this updated file with Pandas, it should mostly be a matter of updating field names throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Magic function to enable interactive plotting (zoom/pan) in Jupyter notebook\n",
    "#If running locally, this would be `%matplotlib notebook`, but since we're using Juptyerlab, we use widget\n",
    "#%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path to sample GLAS data\n",
    "glas_fn = 'GLAH14_tllz_conus_lulcfilt_demfilt.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick check of csv file contents\n",
    "!head $glas_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "Trust me, you should learn how to use `Pandas`, regardless of your ICESat-2 application.  \n",
    "\n",
    "A significant portion of the Python data science ecosystem is based on Pandas and/or Pandas data models.\n",
    "\n",
    ">pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with \"relational\" or \"labeled\" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way towards this goal.\n",
    "\n",
    "https://github.com/pandas-dev/pandas#main-features\n",
    "\n",
    "If you are working with tabular data, especially time series data, please use pandas.\n",
    "* A better way to deal with tabular data, built on top of NumPy arrays\n",
    "* With NumPy, we had to remember which column number (e.g., 3, 4) represented each variable (lat, lon, glas_z, etc)\n",
    "* Pandas allows you to store data with different types, and then reference using more meaningful labels\n",
    "    * NumPy: `glas_np[:,4]`\n",
    "    * Pandas: `glas_df['glas_z']`\n",
    "* A good \"10-minute\" reference with examples: https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the csv file with Pandas\n",
    "* Note that pandas has excellent readers for most common file formats: https://pandas.pydata.org/pandas-docs/stable/reference/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df = pd.read_csv(glas_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That was easy.  Let's inspect the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data types\n",
    "* Can use the DataFrame `info` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the column labels\n",
    "* Can use the DataFrame `columns` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are new to Python and object-oriented programming, take a moment during the break to consider the difference between the methods and attributes of the DataFrame, and how both are accessed. \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "\n",
    "If this is confusing, ask your neighbor or instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview records using DataFrame `head` and `tail` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the mean and standard deviation for all values in each column\n",
    "* Don't overthink this, should be simple (no loops!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply a custom function to each column\n",
    "* For this example, let's define and apply the Normalized Median Absolute Deviation (NMAD)\n",
    "* https://en.wikipedia.org/wiki/Median_absolute_deviation\n",
    "* For a normal distribution, this is equivalent to the standard deviation. But for data containing outliers, it is a more robust representation of variability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmad(a, c=1.4826):\n",
    "    return np.median(np.fabs(a - np.median(a))) * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.apply(nmad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the NMAD function is now distributed with `scipy.stats`: \n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.median_absolute_deviation.html*\n",
    "\n",
    "Can also apply functions imported from modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.apply(scipy.stats.median_absolute_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print quick stats for entire DataFrame with the `describe` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful, huh?  Note that the `50%` statistic is the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Pandas plotting functionality to create a 2D scatterplot of `glas_z` values\n",
    "* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html\n",
    "* Note that labels and colorbar are automatically plotted!\n",
    "* Adjust the size of the points using the `s=1` keyword\n",
    "* Experiment with different color ramps:\n",
    "    * https://matplotlib.org/examples/color/colormaps_reference.html (I prefer `inferno`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color ramps\n",
    "Information on how to choose a good colormap for your data: https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html  \n",
    "Another great resource (Thanks @fperez!): https://matplotlib.org/cmocean/  \n",
    "**TL;DR** Don't use `jet`, use a perceptually uniform colormap for linear variables like elevation. Use a diverging color ramp for values where sign is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.plot(x='lon', y='lat', kind='scatter', c='glas_z', s=1, cmap='inferno');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment by changing the variable represented with the color ramp\n",
    "* Try `decyear` or other columns to quickly visualize spatial distribution of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.plot(x='lon', y='lat', kind='scatter', c='decyear', s=1, cmap='inferno');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left off here 6/4/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a histogram of GLAS elevation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.hist('glas_z', bins=128);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on vertical datums\n",
    "\n",
    "Note that some elevations are less than 0 m.  How can this be?\n",
    "\n",
    "These are height values above (or below) the WGS84 ellipsoid.  Is this the same thing as mean sea level?\n",
    "\n",
    "https://vdatum.noaa.gov/docs/datums.html\n",
    "\n",
    "![Geodetic vs. Orthometric height](https://geodesy.noaa.gov/GEOID/IMAGES/d05.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check spatial distribution of points below 0 (height above WGS84 ellipsoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = glas_df[glas_df['glas_z'] < 0].plot(x='lon', y='lat', kind='scatter', c='dem_z', s=1, cmap='inferno', vmin=-30, vmax=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geoid offset\n",
    "Height differene between the WGS84 ellipsoid (simple shape model of the Earth) and a geoid, that approximates a geopotential (gravitational) surface, approximately mean sea level.\n",
    "\n",
    "EGM96 geoid offset grid\n",
    "\n",
    "https://upload.wikimedia.org/wikipedia/commons/3/3b/Earth_Gravitational_Model_1996.png\n",
    "\n",
    "![EGM96 geoid offset grid](https://upload.wikimedia.org/wikipedia/commons/3/3b/Earth_Gravitational_Model_1996.png)\n",
    "\n",
    "Note values for CONUS.  \n",
    "\n",
    "A lot of the points with elevation < 0 m in the above plot are near coastal sites, roughly near mean sea level.  We see that the geoid offset (difference between WGS84 ellipsoid and EGM96 geoid in this case) for CONUS is roughly -20 m.  So the ICESat GLAS point elevations relative to the ellipsoid near the coast are roughly -20 m, even though they are 0 m relative to the geoid (approximately mean sea level)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the elevation difference between ICESat glas_z and SRTM dem_z values\n",
    "\n",
    "Earlier, I mentioned that I had sampled the SRTM-GL1 DEM for each GLAS shot.  Let's compute the difference and store in a new column in our DataFrame called `glas_srtm_dh`\n",
    "\n",
    "Remember the order of this calculation (if output difference values are negative, which dataset is higher?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df['glas_srtm_dh'] = glas_df['glas_z'] - glas_df['dem_z']\n",
    "glas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the difference between ICESat point timestamp and SRTM timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#February 11-22, 2000\n",
    "srtm_decyear = 2000.112\n",
    "glas_df['glas_srtm_dt'] = glas_df['decyear'] - srtm_decyear\n",
    "glas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute (apparent) elevation change rates between 2000 and 2003-2009 (m/yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df['glas_srtm_dhdt'] = glas_df['glas_srtm_dh']/glas_df['glas_srtm_dt']\n",
    "glas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a scatterplot of difference values\n",
    "* Use a symmetrical `RdBu` (Red to Blue) colormap\n",
    "* Set the colormap limits using `vmin` and `vmax` keyword arguments to be symmetrical about 0 with appropriate range to bring out relevant detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = glas_df.plot(x='lon', y='lat', kind='scatter', c='glas_srtm_dh', s=1, cmap='RdBu', vmin=-10, vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = glas_df.plot(x='lon', y='lat', kind='scatter', c='glas_srtm_dh', s=1, cmap='RdBu', vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a histogram of the difference values\n",
    "* Increase the number of bins, and limit the range to something reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = glas_df.hist('glas_srtm_dh', bins=128, range=(-20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a scatterplot of elevation difference vs. elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = glas_df.plot('glas_z', 'glas_srtm_dh', kind='scatter', s=1)\n",
    "#Add a horizontal line at 0\n",
    "ax.axhline(0, color='k', lw=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers\n",
    "The initial filter in `glas_proc.py` removed GLAS points with absolute elevation difference >200 m compared to the SRTM elevations.  We expect most real elevation change signals to be less than this for the given time period.  But clearly some outliers remain.\n",
    "\n",
    "Use a simple sigma filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean difference:\", glas_df['glas_srtm_dh'].mean())\n",
    "thresh = 3.5 * glas_df['glas_srtm_dh'].std()\n",
    "print(\"3.5 * std:\", thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (glas_df['glas_srtm_dh'] - glas_df['glas_srtm_dh'].mean()).abs() <= thresh\n",
    "glas_df_fltr = glas_df[idx]\n",
    "print(\"Number of points before filter:\", glas_df.shape[0])\n",
    "print(\"Number of points after filter:\", glas_df_fltr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim = thresh\n",
    "f, axa = plt.subplots(1,2, figsize=(10,4))\n",
    "glas_df.plot(ax=axa[1], x='glas_z', y='glas_srtm_dh', kind='scatter', s=1, color='k', label='Outliers')\n",
    "glas_df_fltr.plot(ax=axa[0], x='lon', y='lat', kind='scatter', c='glas_srtm_dh', s=1, cmap='RdBu', vmin=-clim, vmax=clim)\n",
    "glas_df_fltr.plot(ax=axa[1], x='glas_z', y='glas_srtm_dh', kind='scatter', s=1, c='orange', label='Inliers')\n",
    "glas_df[~idx].plot(ax=axa[0], x='lon', y='lat', kind='scatter', color='k', s=1, legend=False)\n",
    "axa[1].axhline(0,color='k')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active remote sensing sanity check\n",
    "\n",
    "OK, we removed some outliers.  But there are still some big differences between the SRTM and GLAS elevation.  \n",
    "\n",
    "* Do you see systematic differences between the glas_z and dem_z values?\n",
    "* Any clues from the scatterplot? (e.g., do some tracks (north-south lines of points) display systematic bias?)\n",
    "* Brainstorm some ideas about what might be going on here.  Think about the nature of each sensor:\n",
    "    * ICESat was a Near-IR laser (1064 nm wavelength) with a big ground spot size (~70 m in diameter)\n",
    "        * Timestamps span different seasons between 2003-2009\n",
    "    * SRTM was a C-band radar (5.3 GHz, 5.6 cm wavelength) with approximately 30 m ground sample distance (pixel size)\n",
    "        * Timestamp was February 2000\n",
    "        * Data gaps (e.g., radar shadows, steep slopes) were filled with ASTER GDEM2 composite, which blends DEMs acquired over many years ~2000-2014\n",
    "* Consider different surfaces and how the laser/radar footprint might be affected:\n",
    "    * Flat bedrock surface\n",
    "    * Dry sand dunes\n",
    "    * Steep montain topography like the Front Range in Colorado  \n",
    "    * Dense vegetation of the Hoh Rainforest in Olympic National Park"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geopandas\n",
    "\n",
    "pandas is great, but what if we want to do some geospatial operations - like compute the intersection between Point and Polygon features?\n",
    "\n",
    "Enter Geopandas - all the great things about pandas, plus geo! (http://geopandas.org/).\n",
    "\n",
    ">\"GeoPandas is an open source project to make working with geospatial data in python easier. GeoPandas extends the datatypes used by pandas to allow spatial operations on geometric types. Geometric operations are performed by shapely. Geopandas further depends on fiona for file access and descartes and matplotlib for plotting.\"\n",
    "\n",
    ">\"GeoPandas enables you to easily do operations in python that would otherwise require a spatial database such as PostGIS.\"\n",
    "\n",
    "Under the hood, GeoPandas is pandas plus some other core geospatial packages:\n",
    "* `shapely` for geometry operations (https://shapely.readthedocs.io/en/stable/manual.html)\n",
    "* `fiona` for reading/writing GIS file formats (https://fiona.readthedocs.io/en/latest/manual.html)\n",
    "* `pyproj` for projections and coordinate system transformations (http://pyproj4.github.io/pyproj/stable/)\n",
    "\n",
    "Under those hoods are lower-level geospatial libraries (GEOS, GDAL/OGR, PROJ4) that provide a foundation for most GIS software (open-source and commercial).  I encourage you to explore these - I guarantee you will learn something valuable.\n",
    "\n",
    "For now, let's explore some basic geopandas functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pandas `DataFrame` to geopandas `GeoDataFrame`\n",
    "To do this, we need to create a new column containing standardized `geometry` objects (e.g., `Point`, `Polygon`) for each record in the DataFrame. \n",
    "\n",
    "https://geopandas.readthedocs.io/en/latest/gallery/create_geopandas_from_pandas.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new geometry column containing (x,y) tuples for each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df['geometry'] = list(zip(glas_df['lon'], glas_df['lat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new shapely Point geometry objects from the (lon,lat) tuple\n",
    "Can do this as `Point(lon, lat)`, but let's use Pandas `apply` function to apply to the entire column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df['geometry'] = glas_df['geometry'].apply(Point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create new GeoDataFrame object, passing the updated Pandas DataFrame\n",
    "Define coordinate reference system (4326 is geographic lat/lon on WGS84 Ellispoid)  \n",
    "Note different variable name (`glas_gdf` vs earlier `glas_df`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf = gpd.GeoDataFrame(glas_df, crs={'init' :'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(glas_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(glas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a quick 2D scatterplot\n",
    "\n",
    "Like a Pandas DataFrame, a GeoDataFrame has convenience plotting function that is built on matlplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, looks like a scatterplot.  But let's plot the elevation values with a color ramp.  \n",
    "To do this, just specify the column name as the first argument to `plot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf.plot('glas_z', markersize=1, cmap='inferno', legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGI glacier polygons\n",
    "\n",
    "Let's grab some glacier outline poygons from the Randolph Glacier Inventory (RGI) v6.0: https://www.glims.org/RGI/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the zip file for Region 02 (Western North America)\n",
    "rgi_zip_fn = '02_rgi60_WesternCanadaUS.zip'\n",
    "\n",
    "if not os.path.exists(rgi_zip_fn):\n",
    "    url = 'https://www.glims.org/RGI/rgi60_files/' + rgi_zip_fn\n",
    "    myfile = requests.get(url)\n",
    "    open(rgi_zip_fn, 'wb').write(myfile.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip the file\n",
    "with zipfile.ZipFile(rgi_zip_fn, 'r') as zip_ref:\n",
    "    zip_ref.extractall('rgi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the shapefile filename\n",
    "rgi_fn = 'rgi/02_rgi60_WesternCanadaUS.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RGI shapefile using Geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf = gpd.read_file(rgi_fn)\n",
    "rgi_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default a new integer index is created.  Let's just use the RGI ID as our index\n",
    "rgi_gdf = rgi_gdf.set_index('RGIId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the RGI GeoDataFrame\n",
    "Note the geometry type is POLYGON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a quick plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and plot state polygons for context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, let's see.  Two choices:\n",
    "1. We could go to ESRI or the U.S. Census website, identify and download a shapefile, unzip 4+ files, copy/paste the appropriate \\*.shp filename into the notebook.  Wait, how can I download on a remote server?  OK, maybe run something like `wget http://...`, unzip, provide absolute path  \n",
    "*- OR -*\n",
    "2. Give geopandas a url that points to a GeoJSON file somewhere on the web, and read dynamically\n",
    "\n",
    "Yeah, let's go with #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the US States 5M GeoJSON here: http://eric.clst.org/tech/usgeojson/\n",
    "\n",
    "We've heard GeoJSON mentioned a few times this week.  It's a great format.  If you are unfamiliar: https://en.wikipedia.org/wiki/GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1:5000000 scale polygons\n",
    "states_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\n",
    "#1:500000 scale polygons (larger file, more vertices)\n",
    "#states_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_500k.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_gdf = gpd.read_file(states_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect State GeoDataFrame\n",
    "Note that some geometry entries are tuples of POLYGON objects - these are states with islands or rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our quick plot of RGI polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "ax = world.plot(alpha=0.2)\n",
    "states_gdf.plot(ax=ax, facecolor='none', edgecolor='0.5', linewidth=0.5)\n",
    "rgi_gdf.plot(ax=ax, edgecolor='k', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip RGI polygons to CONUS\n",
    "GeoPandas makes spatial selection easy.  \n",
    "\n",
    "We'll discuss two options: 1) using a bounding box, and 2) using a polygon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf.total_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define min/max variables for each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, ymin, xmax, ymax = glas_gdf.total_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new GeoDataFrame from output of simple spatial filter with GeoPandas `cx` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_conus = rgi_gdf.cx[xmin:xmax, ymin:ymax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick plot to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_conus.plot(edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clip points to arbitrary Polygon geometry\n",
    "\n",
    "Let's define a Polygon around our area of interest (the GLAS points) \n",
    "\n",
    "To do this, we'll first take the union of our ~65K GLAS points, and then compute the convex hull\n",
    "\n",
    "This will return a Polygon geometry object, which renders nicely in the Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_chull = glas_gdf.unary_union.convex_hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the type\n",
    "type(glas_gdf_chull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview geometry\n",
    "Note that geometry objects (points, lines, polygons, etc.) will render directly in the Jupyter notebook!  Great for quick previews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_chull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute intersection between all RGI polygons and the convex hull\n",
    "Use the GeoDataFrame `intersects()` function.  \n",
    "This will return a Boolean DataSeries, True if points intersect the polygon, False if they do not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_idx = rgi_gdf.intersects(glas_gdf_chull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_idx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract records with True for the intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of RGI polygons before:\",rgi_gdf.shape[0])\n",
    "rgi_gdf_conus = rgi_gdf[rgi_gdf_idx]\n",
    "print(\"Number of RGI polygons after:\", rgi_gdf_conus.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick plot to verify\n",
    "Note latitude range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_conus.plot(edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above examples used standard geodetic lat/lon (EPSG:4326).  This is fine for global analyses and basic visualization.  But as you know, the width of a degree of longitude varies with latitude (~111 km near equator, ~0 m near pole), so we have a scaling problem.\n",
    "\n",
    "So we need to choose a map projection that is appropriate for our data.  This decision is important for visualization, but is also critical for precise quantitative analysis.  For example, if you want to compute volume change, you should use an equal-area projection.  If you want to calculate true distances between two points, you should use an equidistant projection.  \n",
    "\n",
    "https://www.axismaps.com/guide/general/map-projections/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a projection for CONUS\n",
    "\n",
    "Let's use a custom Albers Equal Area projection to minimize distoration over the full spatial extent of our GLAS points\n",
    "\n",
    "To do this, we'll define a PROJ4 string (https://proj.org/usage/quickstart.html?highlight=definition), which can be interpreted by most Python geopackages (like `pyproj`).\n",
    "\n",
    "The Albers Equal Area projection requires two standard parallels: https://proj.org/operations/projections/aea.html.  We'll use a center lat/lon here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conus_aea_proj4 = '+proj=aea +lat_1=37.31 +lat_2=46.69 +lat_0=42.00 +lon_0=-114.27 +ellps=WGS84 +units=m +no_defs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproject the GLAS points\n",
    "\n",
    "This is simple with the GeoDataFrame `to_crs()` method, which uses pyproj under the hood.\n",
    "\n",
    "We'll store as a new GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea = glas_gdf.to_crs(conus_aea_proj4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the updated geometry\n",
    "Note the change in coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the projected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axa = plt.subplots(1,2, figsize=(9,4))\n",
    "glas_gdf.plot(ax=axa[0], column='glas_z', cmap='inferno', markersize=1, legend=True)\n",
    "axa[0].set_title('WGS84')\n",
    "glas_gdf_aea.plot(ax=axa[1], column='glas_z', cmap='inferno', markersize=1, legend=True)\n",
    "axa[1].set_title('Albers Equal-area')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, great, but what did we just do?\n",
    "\n",
    "Under the hood, GeoPandas used the pyproj library (a Python API for PROJ) to transform each point from one coordinate system to another coordinate system.  \n",
    "\n",
    "You've all done this kind of thing before: https://en.wikipedia.org/wiki/List_of_common_coordinate_transformations\n",
    "\n",
    "In 2D, transforming (x,y) coordinates between different projections (e.g., WGS84 vs. Albers Equal Area) on the same reference ellipsoid is pretty straightforward.  Things start to get more complicated when you include different ellipsoid models, horizontal/vertical datums, etc.  Oh, also the Earth's surface is not static - plate tectonics make everything more complicated, as time becomes important, and transformations must include a \"kinematic\" component.  \n",
    "\n",
    "Fortunately, the PROJ library (https://proj4.org/about.html) has generalized much of the complicated math for geodetic coordinate transformations.  It's been under development since the 1980s, and our understanding of the Earth's shape and plate motions has changed dramatically in that time period.  So, still pretty complicated, and there are different levels of support/sophistication in the tools/libraries that use PROJ (like fiona or GeoPandas).\n",
    "\n",
    "We aren't going to get into the details here, but feel free to take a look at the Transformations section here to get a sense of how this is actually accomplished: https://proj4.org/operations/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = glas_gdf_aea.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject the state and RGI polygons\n",
    "states_gdf_aea = states_gdf.to_crs(conus_aea_proj4)\n",
    "rgi_gdf_conus_aea = rgi_gdf_conus.to_crs(conus_aea_proj4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = states_gdf_aea.plot(edgecolor='k', facecolor='none', lw=0.5)\n",
    "rgi_gdf_conus_aea.plot(ax=ax, edgecolor='k', lw=0.5, alpha=0.5)\n",
    "glas_gdf_aea.plot(ax=ax, column='glas_z', cmap='inferno', markersize=0.1, legend=True)\n",
    "#glas_gdf_aea.plot(ax=ax, column='decyear', cmap='inferno', markersize=1, legend=True)\n",
    "ax.set_xlim(bbox[[0,2]])\n",
    "ax.set_ylim(bbox[[1,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hexbin plots\n",
    "\n",
    "Hexbin plots are great for visualizing spatial distribution of binned point density or other metric (e.g., bin median). \n",
    "\n",
    "They are preferable over a standard square/rectangular grid: https://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/h-whyhexagons.htm\n",
    "\n",
    "Here are some resources on generating hexbins using Python and matplotlib:\n",
    "* https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hexbin.html\n",
    "* http://darribas.org/gds15/content/labs/lab_09.html\n",
    "\n",
    "Note: an equal-area projection is also a good idea for a hexbin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate number of bins dynamically with specified bin spacing in meters\n",
    "#bin_width = 27000 #meters\n",
    "#nbins_x = int(np.ceil(np.abs(bbox[2] - bbox[0])/bin_width))\n",
    "#nbins_y = int(np.ceil(np.abs(bbox[3] - bbox[1])/bin_width))\n",
    "#print(nbins_x, nbins_y)\n",
    "#nbins = nbins_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(8,6))\n",
    "hb = ax.hexbin(glas_gdf_aea.geometry.x, glas_gdf_aea.geometry.y, gridsize=nbins, cmap='inferno', bins='log', alpha=0.6, mincnt=1)\n",
    "plt.colorbar(hb, label='Point Count')\n",
    "states_gdf_aea.plot(ax=ax, facecolor='none', edgecolor='black');\n",
    "ax.set_xlim(bbox[[0,2]]);\n",
    "ax.set_ylim(bbox[[1,3]]);\n",
    "ax.set_title('GLAS Bin Point Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(8,6))\n",
    "hb = ax.hexbin(glas_gdf_aea.geometry.x, glas_gdf_aea.geometry.y, C=glas_gdf_aea['glas_z'], \\\n",
    "               reduce_C_function=np.median, gridsize=nbins, cmap='inferno', alpha=0.6)\n",
    "plt.colorbar(hb, ax=ax, label='Elevation (m HAE)')\n",
    "states_gdf_aea.plot(ax=ax, facecolor='none', edgecolor='black');\n",
    "ax.set_xlim(bbox[[0,2]]);\n",
    "ax.set_ylim(bbox[[1,3]]);\n",
    "ax.set_title('GLAS Bin Median Elevation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Analysis of (apparent) elevation change over CONUS glaciers\n",
    "\n",
    "Let's try to consider the following:\n",
    "\n",
    "*Can we identify CONUS glacier surface elevation change that occurred between SRTM (2000) and GLAS (2003-2009) data collection?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge GLAS points with RGI polygons\n",
    "Earlier, we selected RGI polygons using a bounding box, and then did a simple intersection with a polygon.  \n",
    "\n",
    "Now, let's find the GLAS points that intersect **each** RGI glacier polygon.  \n",
    "\n",
    "One approach would be to loop through each glacier polygon, and do an intersection operation like we did with the convex hull.  But this is inefficient, and won't scale well.  It is much more efficient to do a spatial join between the points and the polygons with the intersection operator.  \n",
    "\n",
    "You all probably learned how to perform a join or spatial join in a GIS.  So, do we need to open ArcMap or QGIS here?  Do we need a full-fledged spatial database like PostGIS?  No!  GeoPandas has you covered.\n",
    "\n",
    "http://geopandas.org/reference/geopandas.sjoin.html#geopandas.sjoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To simplify, let's isolate relevant columns in RGI GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_conus_aea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just use these columns from RGI\n",
    "rgi_col = ['Area','Name','geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the spatial join\n",
    "We'll use the intersection operation, and only return points that actually intersect glacier polygons.  Let's store this in a new GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi = gpd.sjoin(glas_gdf_aea, rgi_gdf_conus_aea[rgi_col], op='intersects', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The join uses \"index_right\" for the RGI index column name.  Let's rename to RGIId\n",
    "glas_gdf_aea_rgi = glas_gdf_aea_rgi.rename(columns={\"index_right\": \"RGIId\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of RGI polygons before:\", rgi_gdf_conus_aea.shape[0])\n",
    "print(\"Number of GLAS points before:\", glas_gdf_aea.shape[0])\n",
    "print(\"Number of GLAS points that intersect RGI polygons:\", glas_gdf_aea_rgi.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check number of GLAS points per RGI polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi['RGIId'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which glacier has the greatest number of points?\n",
    "\n",
    "Some notes on indexing and selecting from Pandas DataFrame: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html\n",
    "\n",
    "https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/\n",
    "\n",
    "Here we'll use the `loc` function to pull out the record for the RGIId key with the highest point count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = glas_gdf_aea_rgi['RGIId'].value_counts().index[0]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_conus_aea.loc[label]['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_conus_aea.loc[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_conus_aea.loc[label].geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ipyleaflet plot\n",
    "\n",
    "OK, great, but where is this glacier?  Let's plot on an interactive ipyleaflet map.\n",
    "\n",
    "Note that leaflet uses tiled basemaps: https://en.wikipedia.org/wiki/Tiled_web_map \n",
    "\n",
    "Default projection is Web Mercator (EPSG:3857): https://en.wikipedia.org/wiki/Web_Mercator_projection.  This works well for lower latitudes, but not the polar regions.  I know QGIS can reproject tiled basemaps on the fly - I haven't tested with ipyleaflet.\n",
    "\n",
    "FYI, `folium` provides similar functionality outside of the iPython/Jupyter stack: https://python-visualization.github.io/folium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, Marker, basemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at all of the basemap options!\n",
    "basemaps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = (rgi_gdf_conus_aea.loc[label]['CenLat'], rgi_gdf_conus_aea.loc[label]['CenLon'])\n",
    "basemap = basemaps.Stamen.Terrain\n",
    "m = Map(center=center, zoom=12, basemap=basemap)\n",
    "#label=rgi_gdf_conus_aea.loc[label]['Name']\n",
    "marker = Marker(location=center, draggable=True)\n",
    "m.add_layer(marker);\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot points and RGI polygons, Zoom in on WA state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim = (-3.0, 3.0)\n",
    "ax=states_gdf_aea.plot(facecolor='none', edgecolor='black');\n",
    "rgi_gdf_conus_aea.plot(ax=ax, edgecolor='k', lw=0.5, alpha=0.1);\n",
    "glas_gdf_aea.plot(ax=ax, column='glas_srtm_dhdt', cmap='RdBu', markersize=1, legend=True, vmin=clim[0], vmax=clim[1])\n",
    "ax.set_xlim(bbox[[0,2]]);\n",
    "ax.set_ylim(bbox[[1,3]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby and Aggregate\n",
    "\n",
    "OK, so we know that our sampling isn't great and our dh/dt values are noisy.  But we're here to learn some core concepts and tools, so let's compute some statistics for each glacier anyway.  Hopefully you'll see the value of these operations, and be able to reproduce when you do have a good sample.\n",
    "\n",
    "We can use the Pandas Groupby functionality to group GLAS points for each RGI polygon, and then aggregate using different functions (e.g., mean, std) for different attributes (e.g., 'glas_z', 'glas_srtm_dhdt').\n",
    "\n",
    "This concept can feel a bit abstract at first, but it is very powerful.  \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi.groupby('RGIId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm.  Nothing happened.  Ah, we need a function to perform the aggregation over the grouped data!  How about taking the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi.groupby('RGIId').mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi.groupby('RGIId').std().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a more sophisticated aggregation function\n",
    "\n",
    "A dictionary of fields and functions can be used to compute a set of summary statistics for relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {'Name':'first',\n",
    "            'Area':'first', \n",
    "            'glas_z':['mean', 'median', 'std'],\n",
    "            'glas_srtm_dhdt':['count','mean', 'median', 'std']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi_agg = glas_gdf_aea_rgi.groupby('RGIId').agg(agg_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need the multi-index here\n",
    "glas_gdf_aea_rgi_agg.columns = ['_'.join(col).rstrip('_') for col in glas_gdf_aea_rgi_agg.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi_agg['glas_pt_density'] = glas_gdf_aea_rgi_agg['glas_srtm_dhdt_count']/glas_gdf_aea_rgi_agg['Area_first']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait a minute, what happened to our polygon geometry?\n",
    "\n",
    "This was a casualty of our initial spatial join, as we preserved the Point geometry for each GLAS record, not the RGI geometry.\n",
    "\n",
    "Let's create a new GeoDataFrame, adding the original RGI geometry to the aggregated statistics.\n",
    "\n",
    "Since both DataFrames have the same Index (RGIId), Pandas will automatically join with corresponding records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_gdf_conus_aea['geometry'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi_agg_gdf = gpd.GeoDataFrame(glas_gdf_aea_rgi_agg, geometry=rgi_gdf_conus_aea['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi_agg_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make some final maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some other useful mapping packages\n",
    "\n",
    "`matplotlib-scalebar` adds a dynamic scalebar to matplotlib axes\n",
    "\n",
    "`contextily` downloads and statically renders basemap tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_basemap(ax, zoom, url='http://tile.stamen.com/terrain/tileZ/tileX/tileY.png'):\n",
    "    xmin, xmax, ymin, ymax = ax.axis()\n",
    "    #Prepare the context basemap tiles\n",
    "    basemap, extent = ctx.bounds2img(xmin, ymin, xmax, ymax, zoom=zoom, url=url)\n",
    "    #Add the basemap to axes\n",
    "    ax.imshow(basemap, extent=extent, interpolation='bilinear')\n",
    "    #Restore original x/y limits\n",
    "    ax.axis((xmin, xmax, ymin, ymax))\n",
    "    #Create a scalebar object, with scaling factor of 1.0 px, since we're using projected coordinate system with unit 1 m\n",
    "    scalebar = ScaleBar(1.0)\n",
    "    #Add scalebar to axes\n",
    "    ax.add_artist(scalebar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to web mercator (EPSG:3857) to match format of tiled basemap\n",
    "glas_gdf_aea_rgi_agg_gdf_wm = glas_gdf_aea_rgi_agg_gdf.to_crs(epsg=3857)\n",
    "glas_gdf_aea_rgi_wm = glas_gdf_aea_rgi.to_crs(epsg=3857)\n",
    "rgi_gdf_conus_aea_wm = rgi_gdf_conus_aea.to_crs(epsg=3857)\n",
    "states_gdf_aea_wm = states_gdf.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim = (-5.0, 5.0)\n",
    "f, axa = plt.subplots(1,2, figsize=(10,4), sharex=True, sharey=True)\n",
    "\n",
    "rgi_gdf_conus_aea_wm.plot(ax=axa[0], edgecolor='k', facecolor='w', lw=0.5, alpha=0.2);\n",
    "glas_gdf_aea_rgi_agg_gdf_wm.plot(ax=axa[0],column='glas_srtm_dhdt_count', cmap='inferno', edgecolor='k', lw=0.5, legend=True)\n",
    "axa[0].set_title('Number of GLAS shots per RGI Polygon')\n",
    "add_basemap(axa[0], zoom=5)\n",
    "\n",
    "glas_gdf_aea_rgi_agg_gdf_wm.plot(ax=axa[1],column='glas_srtm_dhdt_median', cmap='RdBu', edgecolor='k', lw=0.5, vmin=clim[0], vmax=clim[1], legend=True)\n",
    "axa[1].set_title('Median SRTM to GLAS dh/dt (m/yr)')\n",
    "glas_gdf_aea_rgi_wm.plot(ax=axa[1],column='glas_srtm_dhdt', cmap='RdBu', markersize=5, edgecolor='0.5', lw=0.5, vmin=clim[0], vmax=clim[1])\n",
    "states_gdf_aea_wm.plot(ax=axa[1], edgecolor='k', facecolor='none', lw=0.5)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom to North Cascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we extract any additional insight from the reduced data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point density vs. Glacier Area\n",
    "\n",
    "Do we see more valid GLAS points over bigger glaciers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.scatter(glas_gdf_aea_rgi_agg_gdf['Area_first'], glas_gdf_aea_rgi_agg_gdf['glas_srtm_dhdt_count'])\n",
    "ax.set_ylabel('Number of GLAS shots')\n",
    "ax.set_xlabel('Glacier Area (km^2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median dh/dt vs. Glacier Area\n",
    "\n",
    "Do we see less elevation change over bigger glaciers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.scatter(glas_gdf_aea_rgi_agg_gdf['Area_first'], glas_gdf_aea_rgi_agg_gdf['glas_srtm_dhdt_median'])\n",
    "ax.set_xlabel('Glacier Area (km^2)')\n",
    "ax.set_ylabel('Median SRTM - GLAS dh/dt (m/yr)')\n",
    "ax.axhline(0, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median dh/dt vs. Glacier Elevation\n",
    "\n",
    "Do we see greater elevation loss at lower elevations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.scatter(glas_gdf_aea_rgi_agg_gdf['glas_z_median'], glas_gdf_aea_rgi_agg_gdf['glas_srtm_dhdt_median'])\n",
    "ax.set_xlabel('Median Glacier Elevation (m HAE)')\n",
    "ax.set_ylabel('Median SRTM - GLAS dh/dt (m/yr)')\n",
    "ax.axhline(0, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "### ¯\\\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save output as GIS-ready file\n",
    "\n",
    "The workflows we're developing with Jupyter Notebooks are intended to be all-inclusive and reproducible - they start by reading raw data and end with final results.\n",
    "\n",
    "But sometimes you want to write out geospatial data for analysis in a GUI-based GIS (QGIS, ArcMap) to share with colleagues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose a format?\n",
    "Under the hood, geopandas uses the `fiona` package for reading/writing vector data\n",
    "\n",
    "`fiona.supported_drivers`\n",
    "\n",
    "```\n",
    "{'ARCGEN': 'r',\n",
    " 'AeronavFAA': 'r',\n",
    " 'BNA': 'raw',\n",
    " 'CSV': 'raw',\n",
    " 'DGN': 'raw',\n",
    " 'DXF': 'raw',\n",
    " 'ESRI Shapefile': 'raw',\n",
    " 'GML': 'raw',\n",
    " 'GPKG': 'rw',\n",
    " 'GPSTrackMaker': 'raw',\n",
    " 'GPX': 'raw',\n",
    " 'GeoJSON': 'rw',\n",
    " 'Idrisi': 'r',\n",
    " 'MapInfo File': 'raw',\n",
    " 'OpenFileGDB': 'r',\n",
    " 'PCIDSK': 'r',\n",
    " 'S57': 'r',\n",
    " 'SEGY': 'r',\n",
    " 'SUA': 'r'}\n",
    " ```\n",
    "\n",
    "* You've all used shapefiles in the past.  Please stop.  This is a legacy format, though it is still widely used.\n",
    "* http://switchfromshapefile.org/\n",
    "* Better options these days are Geopackage (GPKG) when spatial index is required, and GeoJSON for most cases - both should be supported by any respectable GIS\n",
    "* Let's use geopackage for this exercise\n",
    "* Now that you've made an informed decision, pick a filename and use the Geopandas `to_file()` method\n",
    "    * Make sure you properly specify filename with extension and the `driver` option\n",
    "    * Note: Writing out may take a minute, and may produce an intermediate '.gpkg-journal' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi_agg_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_aea_rgi_agg_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn='./conus_glas_gdf_aea_rgi_agg.gpkg'\n",
    "glas_gdf_aea_rgi_agg_gdf.to_file(out_fn, driver='GPKG')\n",
    "\n",
    "#out_fn='./conus_glas_gdf_aea_rgi_agg.geojson'\n",
    "#glas_gdf_aea_rgi_agg_gdf.to_file(out_fn, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## :tada:\n",
    "\n",
    "You can now directly load this gpkg file in any GIS, without defining a coordinate system or dealing with your original csv.  \n",
    "\n",
    "Try it! Right-click on file in the file browser (left side of the JupyterLab interface) -> Download.  Then open in QGIS on your local machine.\n",
    "\n",
    "You can also load this file directly into geopandas in the future using the `read_file()` method, without having to do any of the processing above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, what if we actually had a decent GLAS sample of CONUS glaciers?\n",
    "\n",
    "## Estimate glacier mass balance\n",
    "\n",
    "We could have estimated mass balance for each glacier polygon using the mean dh/dt, glacier area, and a standard bulk density (850 kg/m3, Huss, 2013).  Could then perform a secondary aggregation to compile statistics for mountain ranges, watersheds, etc.\n",
    "\n",
    "With sparse sample, probably best to bin points to derive dh/dt vs. elevation curves, then combine with observed glacier hypsometry to estimate mass balance for glacier polygons.  This can work, but need to be careful about spatial variability.  Some good papers on this for HMA and other regions led by Andy Kaab and Alex Gardner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final thoughts\n",
    "\n",
    "GLAS was not the right tool for small, low-latitude CONUS glaciers.  We kind of knew this would be the case before we started, but hey, it's worth a look.\n",
    "\n",
    "The concepts and approaches presented here can be applied to larger glaciers or ice caps, especially at higher latitudes.  One could modify to repeat for all RGI regions.\n",
    "\n",
    "We started with an existing csv of culled points here.  One could repeat with a similarly processed subset of ATL06 points using the workflows presented earlier this week.  This will provide a longer time period to evaluate noisy elevation measurements.  Replacing void-filled SRTM with another reference DEM is also needed (e.g., the timestamped USGS NED).\n",
    "\n",
    "Note that the core tools presented here have Dask integration (https://dask.org/) to allow you to chunk and process larger-than-memory datasets with minimal modification to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Distortion in Polar Stereographic Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The [polar stereographic] projection is defined on the entire sphere, except at one point: the projection point. Where it is defined, the mapping is smooth and bijective. It is conformal, meaning that it preserves angles at which curves meet. It is neither isometric nor area-preserving: that is, it preserves neither distances nor the areas of figures.\" [Wikipedia]\n",
    "\n",
    "Most of you are probably using polar stereographic projections, likely the standard EPSG:3031 or EPSG:3413 projections for Antarctica and Greenland, respectively.  These are designed to minimize distortion near the latitude of true scale (71°S for Antarctica and 70°N for Greenland).  This means that area and distance distortion will increase as you move away from this latitude.  So areas or distances measured near the pole will not be equivalent to areas or distances measured on the Antarctic Peninsula.  The difference isn't huge, but is nontrivial (~1-2% for Antarctica I believe).  See https://nsidc.org/data/polar-stereo/ps_grids.html for more details.\n",
    "\n",
    "The figures here might help illustrate: https://en.wikipedia.org/wiki/Stereographic_projection#Properties\n",
    "\n",
    "Let's try to illustrate this nuanced, but often overlooked issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tissot's Indicatrix for polar stereographic projection\n",
    "\n",
    "Let's use the classic Tissot's Indicatrix plots to show map distortion.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tissot%27s_indicatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "#Cartopy implementation of EPSG:3031 and EPSG:3413\n",
    "crs_3031 = ccrs.SouthPolarStereo(true_scale_latitude=-71)\n",
    "crs_3413 = ccrs.NorthPolarStereo(central_longitude=-45, true_scale_latitude=70)\n",
    "\n",
    "#Circle locations\n",
    "lons = range(-180, 180, 30)\n",
    "lats = range(-90, 91, 10)\n",
    "#Radius of circles\n",
    "rad = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1, 2, 1, projection=crs_3413)\n",
    "ax1.coastlines()\n",
    "ax1.gridlines(ylocs=[70,],color='r')\n",
    "ax1.tissot(facecolor='orange', edgecolor='0.5', alpha=0.4, rad_km=rad, lons=lons, lats=lats)\n",
    "ax1.set_title('EPSG:3413')\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2, projection=crs_3031)\n",
    "ax2.coastlines()\n",
    "ax2.gridlines(ylocs=[-71,],color='r')\n",
    "ax2.tissot(facecolor='orange', edgecolor='0.5', alpha=0.4, rad_km=rad, lons=lons, lats=lats)\n",
    "ax2.set_title('EPSG:3031')\n",
    "ax2.set_extent([-180, 180, -90, -50], ccrs.PlateCarree())\n",
    "\n",
    "#There is a bug in cartopy extent when central_longitude is not 0\n",
    "#Get extent in projected crs (x0, x1, y0, y1)\n",
    "extent_3413 = ax2.get_extent()\n",
    "ax1.set_extent(extent_3413, crs_3413)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, cool plot bro.  But why does this matter?\n",
    "\n",
    "Note the size of the circles in the corners and over the pole, relative to the circles near the latitude of true scale (red line).  While it is unlikely that you'll use this projection to look at mid-latitude regions, you can see the difference in area distortion over the ~20° of latitude between North and South Greenland.\n",
    "\n",
    "Say you generated amazing elevation difference grids for all of the Greenland ice sheet using ICESat and ICESat-2 crossovers, and you used the EPSG:3413 projection.  Say the grids have a posting (grid cell size) of 120 m.  Imagine didividing the above plots into grid cells.  You'll end up with more grid cells over the circles at lower latitudes (e.g., South Greenland) and fewer grid cells over circles at high latitudes (e.g., North Greenland).  \n",
    "\n",
    "Let's assume you then integrate your dh/dt values to compute volume change of the Greenland ice sheet by summing dh/dt values for all grid cells in each catchment.  Let's assume all grid cells have the same -1.0 m/yr value.  The integrated estimates for the catchments in South Greenland will have more grid cells, resulting in larger apparent negative volume change!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compare with an equal-area projection\n",
    "Lambert Azimuthal Equal-Area is not a bad choice for the polar regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cartopy implementation of EPSG:3031 and EPSG:3413\n",
    "#crs_slaea = ccrs.LambertAzimuthalEqualArea(central_longitude=0.0, central_latitude=-90.0)\n",
    "crs_nlaea = ccrs.LambertAzimuthalEqualArea(central_longitude=-45.0, central_latitude=90.0)\n",
    "\n",
    "#Specify locations of Tissot's Indicatrix\n",
    "lons = range(-180, 180, 30)\n",
    "lats = range(-90, 91, 10)\n",
    "rad = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a 100 km grid\n",
    "dx = 100000\n",
    "xgrid = np.arange(extent_3413[0], extent_3413[1]+dx, dx)\n",
    "ygrid = np.arange(extent_3413[2], extent_3413[3]+dx, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1, 2, 1, projection=crs_3413)\n",
    "ax1.coastlines()\n",
    "ax1.tissot(facecolor='orange', edgecolor='0.5', alpha=0.4, rad_km=rad, lons=lons, lats=lats)\n",
    "ax1.set_title('EPSG:3413')\n",
    "#ax1.gridlines(crs=ccrs.PlateCarree(), ylocs=lats)\n",
    "ax1.gridlines(crs=crs_3413, xlocs=xgrid, ylocs=ygrid)\n",
    "ax1.gridlines(ylocs=[70,],color='r')\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2, projection=crs_nlaea)\n",
    "ax2.coastlines()\n",
    "ax2.tissot(facecolor='orange', edgecolor='0.5', alpha=0.4, rad_km=rad, lons=lons, lats=lats)\n",
    "ax2.set_title('Lambert Azimuthal Equal-Area')\n",
    "ax2.set_extent([-180, 180, 50, 90], ccrs.PlateCarree())\n",
    "#ax2.gridlines(crs=ccrs.PlateCarree(), ylocs=lats)\n",
    "ax2.gridlines(crs=crs_nlaea, xlocs=xgrid, ylocs=ygrid)\n",
    "\n",
    "#ax1.set_extent(extent_3413, crs_3413)\n",
    "#ax2.set_extent(extent_3413, crs_3413)\n",
    "\n",
    "gr_extent = [-1.4E6,1.4E6,-3.7E6,-5.3E5]\n",
    "ax1.set_extent(gr_extent, crs_3413)\n",
    "ax2.set_extent(gr_extent, crs_3413)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: the \"Flat Earth\" Projection: Azimuthal Equidistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Azimuthal Equidistant](https://upload.wikimedia.org/wikipedia/commons/2/2f/Flat_earth.png)\n",
    "\n",
    ">An azimuthal equidistant projection of the entire spherical Earth. A rendered picture of the Flat Earth model. The white around the outside of the globe is thought to be an 'Ice Wall', preventing people from falling off the surface of the earth. [Wikipedia]\n",
    "\n",
    "So, those of you who have been to the South Pole, can you help me understand how this works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: Sampling a raster at points\n",
    "\n",
    "This is something that is surprisingly common, but may not be simple to implement.  Let's discuss a few options:\n",
    "1. Simple `rasterio` sampling with integer indices using nearest neighbor\n",
    "2. Statistics extracted for a circular window around each point location\n",
    "3. NumPy array indices, affine/geotransform between pixel and map coordinates, and NumPy/SciPy interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download sample 90-m SRTM DEM over WA state:  \n",
    "`aws s3 cp s3://pangeo-data-upload-oregon/icesat2/geospatial-analysis/wa_srtm_90m_full_proj.tif .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rasterio sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the file with rasterio\n",
    "\n",
    "https://rasterio.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_fn = 'wa_srtm_90m_full_proj.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is rasterio Dataset object\n",
    "src_proj = rio.open(srtm_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src_proj.profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read band 1 as a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_proj = src_proj.read(1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set nodata values to np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_proj[srtm_proj==src_proj.nodata] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "plt.imshow(srtm_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproject points to match raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raster CRS is UTM 10N (EPSG:32610)\n",
    "src_proj.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_utm = glas_gdf_aea.to_crs(src_proj.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_utm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip points to raster extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, ymin, xmax, ymax = src_proj.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_utm_wa = glas_gdf_utm.cx[xmin:xmax, ymin:ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_utm.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_utm_wa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_utm_wa.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tuples of coordinates from Point geometry in GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_coord = [(pt.x, pt.y) for pt in glas_gdf_utm_wa.geometry]\n",
    "glas_coord[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_sample = list((src_proj.sample(glas_coord, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_sample[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to 1D NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_sample_elev = np.squeeze(srtm_sample).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_sample_elev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set nodata to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_sample_elev[srtm_sample_elev == src_proj.nodata] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtm_sample_elev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to the GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_utm_wa['srtm_90m_z_rio'] = srtm_sample_elev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glas_gdf_utm_wa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Local window sample\n",
    "\n",
    "https://github.com/dshean/demcoreg/blob/master/demcoreg/sample_raster_at_pts.py\n",
    "\n",
    "https://github.com/dshean/pygeotools/blob/master/pygeotools/lib/geolib.py#L1019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab latest dev version of pygeotools repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dshean/pygeotools.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd pygeotools; pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygeotools.lib import geolib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolib.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This doesn't play nicely with rasterio\n",
    "#https://rasterio.readthedocs.io/en/stable/topics/switch.html\n",
    "#from osgeo import gdal\n",
    "#ds = gdal.Open(srtm_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geolib.sample(ds, glas_gdf_utm_wa.geometry.x.values, glas_gdf_utm_wa.geometry.y.values, pad=2, circ=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate shaded relief map for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_fn = os.path.splitext(srtm_fn)[0]+'_hs.tif'\n",
    "!gdaldem hillshade $srtm_fn $hs_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_proj = rio.open(hs_fn)\n",
    "hs = hs_proj.read(1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs[hs==hs_proj.nodata] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sample over hillshade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio.plot\n",
    "f, ax = plt.subplots()\n",
    "ax.imshow(hs, cmap='gray', extent=rio.plot.plotting_extent(hs_proj))\n",
    "glas_gdf_utm_wa.plot('srtm_90m_z_rio', ax=ax, edgecolor='w', cmap='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on sampling coarse rasters or noisy rasters at integer pixel locations\n",
    "* The rasterio approach is efficient, but it uses \"nearest neighbor\" approach to extract the elevation value for the grid cell that contains the point, regradless of where the point falls within the grid cell (center vs. corner)\n",
    "* But our grid cells can be big (~90x90 m for the SRTM3 data)\n",
    "* A better approach would be to use bilinear or bicubic sampling, to interpolate the elevatoin value at the point coordinates\n",
    "* Another approach uses pixels within some radius of the point location, then computes stats (e.g., median elevation)\n",
    "    * https://github.com/dshean/pygeotools/blob/master/pygeotools/lib/geolib.py#L1019\n",
    "    * https://www.earthdatascience.org/courses/earth-analytics-python/lidar-remote-sensing-uncertainty/extract-data-from-raster/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix C: Elevation Point Query APIs\n",
    "\n",
    "New functions in pygeotools geolib\n",
    "\n",
    "Query USGS NED and Open Elevation APIs for lat/lon coordinates  \n",
    "Query NGS geoid offset API for datum conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = glas_gdf_utm_wa.iloc[0:5]['lat'].values\n",
    "lon = glas_gdf_utm_wa.iloc[0:5]['lon'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_z = geolib.get_HAE(lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = glas_gdf_utm_wa.iloc[0:5][['glas_z', 'dem_z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['ned_z'] = ned_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
